# Introduction to Engineering Statistics and R 
Engineering Statistics (IMSE 4410) Spring 2016. 
Copyright 2013-2016 by Timothy Middelkoop License CC by SA 3.0

## Point Estimation - Chapter 7

Estimating a distributions parameters ($\lambda$, a, b, etc) from a sample of
the population is called point estimation.  These estimates are also random 
variables and such have a distribution (with mean and variance).  In most cases
after a sufficiently large (from 4+ to 30+) number of samples the distribution
of the point estimators is normal.  This is a result of the Central Limit Theorem.

An estimator is considered unbiased if the mean of the estimator is the mean
of the sample population.
An estimator that has less variance is considered better than one that has more.

There are a number of techniques of constructing estimators, Method of Moments and
Maximum Likelihood (MLE) for example, and they can be found in the text.
They are methods of finding equations to the parameters of a distribution 
given a sample of a population.  This way we can find the parameters if we do
not know them.

How do we find the MLE (7-4.2)?

We simply take the distribution function, plug in the parameters, and
find the parameters ($\theta$) that _maximizes_ 

$$L(\theta)=f(x_1;\theta) \cdot f(x_2,\theta) \cdot ... \cdot f(x_n;\theta)$$

For the normal distribution the MLE and Method of moments are the same
and are $\mu=\bar x$ and 

$$\sigma^2 = {1 \over n  \sum (x_i-\bar x)^2}$$.

This material is too sophisticated to cover in a single class
Practically we will not be developing our own MLE functions.
Look at section on Point Estimation for more details.  Graduate students will be responsible for this material.

What is important is the fact that point estimators exist, and they are
random variables that tend towards the normal function.

### MLE Uniform Distribution Example

A simple and powerful example is for the uniform distribution.
Assume we have a uniform distribution from 0 to a. From the text the MLE point estimator is $\hat a=max(x)$  We will use A for $\hat a$.

```{r}
# From the text the MLE point estimator is ahat=max(x)
a <- 8 
d <- runif(10,0,a) ; d 

# Plot to make sure we are doing what we expect
hist(d)

# Lets find our post estimator ahat (A)
A <- max(d) ; A

# close
# Lets try for a large n
max(runif(10000,0,a))
# close, sample size has an effect, lets see what.
# Remember the point estimator is a random variable
# A single instance
max(runif(1,0,a))

# replicate 
d <- replicate(100,max(runif(1,0,a))) ; hist(d)
# not very interesting, think what the max of a single number reduces to.

## lets try a few more with larger n
d <- replicate(1000,max(runif(100,0,a))) ; hist(d)

# Those of you who program may find the above line a bit disturbing.
# in the replicate function the second argument is evaluated late (just a bit of magic).

# Here is what the replicate function reduces to. [optional/graduate]
x <- numeric(0)
for (i in 1:1000) x[i] <- max(runif(100,0,a))
hist(x)

```

Things to remember:

* Point estimators estimate parameters to distributions from a sample.
* There are methods to construct point estimators.
* Point estimators are random variables
* To replicate calculations use the replicate function.


